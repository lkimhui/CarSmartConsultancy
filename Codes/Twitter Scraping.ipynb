{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in /usr/local/anaconda3/lib/python3.9/site-packages (0.5.0.20230113)\n",
      "Requirement already satisfied: lxml in /usr/local/anaconda3/lib/python3.9/site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.9/site-packages (from snscrape) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/anaconda3/lib/python3.9/site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/anaconda3/lib/python3.9/site-packages (from snscrape) (2.28.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->snscrape) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## import packages\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: tweets: File exists\r\n"
     ]
    }
   ],
   "source": [
    "## pulling tweets and save to folder\n",
    "# pull via hashtag\n",
    "searchText = \"earthquake\"\n",
    "scraper = sntwitter.TwitterSearchScraper(\"#\" + searchText)\n",
    "\n",
    "# create a directory to hold the files\n",
    "# create a parent folder\n",
    "!mkdir tweets\n",
    "# create a child folder based on search topic\n",
    "filePath = os.path.join(\"tweets\", searchText)\n",
    "if not os.path.exists(filePath):\n",
    "    os.mkdir(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold all text in a list\n",
    "columns = ['date', 'source', 'like', 'retweet', 'content']\n",
    "tweets = []\n",
    "j = 1\n",
    "\n",
    "for i, tweet in enumerate(scraper.get_items()):\n",
    "    \n",
    "    # exit condition\n",
    "    if (i > 1000):\n",
    "        break;\n",
    "        \n",
    "    # scrape relevant info from twitter\n",
    "    data = [tweet.date, tweet.source, tweet.likeCount, tweet.retweetCount, tweet.rawContent]\n",
    "    # add the scraped info to a list\n",
    "    tweets.append(data)\n",
    "\n",
    "    # create a new file for every 10000 of data scraped\n",
    "    if ((i+1) % 500 == 0):\n",
    "        tweets.insert(0, columns)\n",
    "        with open(str(filePath) + \"/copy_\" + str(j) + \".pickle\", \"wb\") as file:\n",
    "            pickle.dump(tweets, file)\n",
    "        tweets.clear()\n",
    "        j+=1\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# count number of sub-files created\n",
    "fileNumber = len(os.listdir('./tweets/' + str(searchText)))\n",
    "print(fileNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweets/earthquake/copy_1.pickle', 'tweets/earthquake/copy_2.pickle']\n"
     ]
    }
   ],
   "source": [
    "# load file object through pickle\n",
    "with open(\"tweets/\"+ searchText + \"/copy_1.pickle\", 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "fileName = []\n",
    "for i in range(fileNumber):\n",
    "    fileName.append(\"tweets/\" + searchText + \"/copy_\" + str(i+1) + \".pickle\")\n",
    "    \n",
    "print(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all files into one dataframe\n",
    "df = pd.concat(map(lambda file : pd.DataFrame(pd.read_pickle(file)[1:], columns=pd.read_pickle(file)[0]), fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# check number of observations \n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 499\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype              \n",
      "---  ------   --------------  -----              \n",
      " 0   date     1000 non-null   datetime64[ns, UTC]\n",
      " 1   source   1000 non-null   object             \n",
      " 2   like     1000 non-null   int64              \n",
      " 3   retweet  1000 non-null   int64              \n",
      " 4   content  1000 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(2)\n",
      "memory usage: 46.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# describe the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>like</th>\n",
       "      <th>retweet</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-23 12:42:34+00:00</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Temblor -  M 4.6 - 89 km WNW of Murghob, Tajik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-23 12:42:01+00:00</td>\n",
       "      <td>&lt;a href=\"http://reddit.com/r/EEW\" rel=\"nofollo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>üåÑ #Sismo! #Earthquake! 4.2 M, 12:20 UTC on lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-23 12:41:48+00:00</td>\n",
       "      <td>&lt;a href=\"https://ifttt.com\" rel=\"nofollow\"&gt;IFT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mandeeptoronto: RT @khalsaaidca: Parampreet Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-23 12:41:42+00:00</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you very much to @eu_echo for allocating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-23 12:41:41+00:00</td>\n",
       "      <td>&lt;a href=\"https://biston.web.id\" rel=\"nofollow\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#Gempa Mag:3.7\\n 23-Feb-2023 19:36:42WIB\\n Lok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  \\\n",
       "0 2023-02-23 12:42:34+00:00   \n",
       "1 2023-02-23 12:42:01+00:00   \n",
       "2 2023-02-23 12:41:48+00:00   \n",
       "3 2023-02-23 12:41:42+00:00   \n",
       "4 2023-02-23 12:41:41+00:00   \n",
       "\n",
       "                                              source  like  retweet  \\\n",
       "0  <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...     0        0   \n",
       "1  <a href=\"http://reddit.com/r/EEW\" rel=\"nofollo...     0        0   \n",
       "2  <a href=\"https://ifttt.com\" rel=\"nofollow\">IFT...     0        0   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...     1        0   \n",
       "4  <a href=\"https://biston.web.id\" rel=\"nofollow\"...     0        0   \n",
       "\n",
       "                                             content  \n",
       "0  Temblor -  M 4.6 - 89 km WNW of Murghob, Tajik...  \n",
       "1  üåÑ #Sismo! #Earthquake! 4.2 M, 12:20 UTC on lan...  \n",
       "2  mandeeptoronto: RT @khalsaaidca: Parampreet Si...  \n",
       "3  Thank you very much to @eu_echo for allocating...  \n",
       "4  #Gempa Mag:3.7\\n 23-Feb-2023 19:36:42WIB\\n Lok...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick peek at the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert UTC datetime to local datetime\n",
    "df['date'] = pd.to_datetime(df['date']).dt.tz_convert('Asia/Singapore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_formatted'] = pd.to_datetime(df['date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>like</th>\n",
       "      <th>retweet</th>\n",
       "      <th>content</th>\n",
       "      <th>date_formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-23 20:42:34+08:00</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Temblor -  M 4.6 - 89 km WNW of Murghob, Tajik...</td>\n",
       "      <td>2023-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-23 20:42:01+08:00</td>\n",
       "      <td>&lt;a href=\"http://reddit.com/r/EEW\" rel=\"nofollo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>üåÑ #Sismo! #Earthquake! 4.2 M, 12:20 UTC on lan...</td>\n",
       "      <td>2023-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-23 20:41:48+08:00</td>\n",
       "      <td>&lt;a href=\"https://ifttt.com\" rel=\"nofollow\"&gt;IFT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mandeeptoronto: RT @khalsaaidca: Parampreet Si...</td>\n",
       "      <td>2023-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-23 20:41:42+08:00</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you very much to @eu_echo for allocating...</td>\n",
       "      <td>2023-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-23 20:41:41+08:00</td>\n",
       "      <td>&lt;a href=\"https://biston.web.id\" rel=\"nofollow\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#Gempa Mag:3.7\\n 23-Feb-2023 19:36:42WIB\\n Lok...</td>\n",
       "      <td>2023-02-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  \\\n",
       "0 2023-02-23 20:42:34+08:00   \n",
       "1 2023-02-23 20:42:01+08:00   \n",
       "2 2023-02-23 20:41:48+08:00   \n",
       "3 2023-02-23 20:41:42+08:00   \n",
       "4 2023-02-23 20:41:41+08:00   \n",
       "\n",
       "                                              source  like  retweet  \\\n",
       "0  <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...     0        0   \n",
       "1  <a href=\"http://reddit.com/r/EEW\" rel=\"nofollo...     0        0   \n",
       "2  <a href=\"https://ifttt.com\" rel=\"nofollow\">IFT...     0        0   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...     1        0   \n",
       "4  <a href=\"https://biston.web.id\" rel=\"nofollow\"...     0        0   \n",
       "\n",
       "                                             content date_formatted  \n",
       "0  Temblor -  M 4.6 - 89 km WNW of Murghob, Tajik...     2023-02-23  \n",
       "1  üåÑ #Sismo! #Earthquake! 4.2 M, 12:20 UTC on lan...     2023-02-23  \n",
       "2  mandeeptoronto: RT @khalsaaidca: Parampreet Si...     2023-02-23  \n",
       "3  Thank you very much to @eu_echo for allocating...     2023-02-23  \n",
       "4  #Gempa Mag:3.7\\n 23-Feb-2023 19:36:42WIB\\n Lok...     2023-02-23  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 499\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype                         \n",
      "---  ------          --------------  -----                         \n",
      " 0   date            1000 non-null   datetime64[ns, Asia/Singapore]\n",
      " 1   source          1000 non-null   object                        \n",
      " 2   like            1000 non-null   int64                         \n",
      " 3   retweet         1000 non-null   int64                         \n",
      " 4   content         1000 non-null   object                        \n",
      " 5   date_formatted  1000 non-null   object                        \n",
      "dtypes: datetime64[ns, Asia/Singapore](1), int64(2), object(3)\n",
      "memory usage: 54.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# check the data type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump dataframe to a pickle file\n",
    "df.to_pickle(\"./tweets/\"+ searchText + \"/1000testset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSection below will reload a dataframe that has been saved to a pickle file\\nRegexp is first implemented to remove special characters from the contents column\\nSpacy-langdetect is then adopted to remove potentially non-english content based on cutoff score of >0.5\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Section below will reload a dataframe that has been saved to a pickle file\n",
    "Regexp is first implemented to remove special characters from the contents column\n",
    "Spacy-langdetect is then adopted to remove potentially non-english content based on cutoff score of >0.5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-langdetect in /usr/local/anaconda3/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: pytest in /usr/local/anaconda3/lib/python3.9/site-packages (from spacy-langdetect) (7.1.2)\n",
      "Requirement already satisfied: langdetect==1.0.7 in /usr/local/anaconda3/lib/python3.9/site-packages (from spacy-langdetect) (1.0.7)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.9/site-packages (from langdetect==1.0.7->spacy-langdetect) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from pytest->spacy-langdetect) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/anaconda3/lib/python3.9/site-packages (from pytest->spacy-langdetect) (1.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/lib/python3.9/site-packages (from pytest->spacy-langdetect) (21.3)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/anaconda3/lib/python3.9/site-packages (from pytest->spacy-langdetect) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/anaconda3/lib/python3.9/site-packages (from pytest->spacy-langdetect) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from pytest->spacy-langdetect) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/anaconda3/lib/python3.9/site-packages (from packaging->pytest->spacy-langdetect) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.11/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/site-packages (from spacy) (1.10.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->spacy) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.11/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/site-packages (from spacy) (1.10.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->spacy) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataframe from pickle file\n",
    "df = pd.read_pickle(\"./tweets/\" + searchText + \"/1000testset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Temblor -  M 4.6 - 89 km WNW of Murghob, Tajik...\n",
       "1      üåÑ #Sismo! #Earthquake! 4.2 M, 12:20 UTC on lan...\n",
       "2      mandeeptoronto: RT @khalsaaidca: Parampreet Si...\n",
       "3      Thank you very much to @eu_echo for allocating...\n",
       "4      #Gempa Mag:3.7\\n 23-Feb-2023 19:36:42WIB\\n Lok...\n",
       "                             ...                        \n",
       "495    6.8 #Earthquake Shakes #Tajikistan https://t.c...\n",
       "496    #Northport send emergency relief for Turkiye, ...\n",
       "497    ‚ö°Ô∏èThe #Kazakh government and #citizens have ex...\n",
       "498    üî¥ #Terremoto magnitudo 6.8 in #Tagikistan: il ...\n",
       "499    Deprem sonrasƒ± evsiz kalan vatanda≈ülarƒ±mƒ±zƒ±n m...\n",
       "Name: content, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning - first round\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[.*?\\,%_-]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>temblor   m    km wnw of murghob tajikistan  e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üåÑ sismo earthquake  m  utc on land r√≠o segundo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mandeeptoronto rt khalsaaidca parampreet singh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you very much to euecho for allocating  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gempa \\n  \\n    km timurlaut deiyaipapua\\n  km...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>earthquake shakes tajikistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>northport send emergency relief for turkiye sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>‚ö°Ô∏èthe kazakh government and citizens have expr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>üî¥ terremoto magnitudo  in tagikistan il moment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>deprem sonrasƒ± evsiz kalan vatanda≈ülarƒ±mƒ±zƒ±n m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content\n",
       "0    temblor   m    km wnw of murghob tajikistan  e...\n",
       "1    üåÑ sismo earthquake  m  utc on land r√≠o segundo...\n",
       "2    mandeeptoronto rt khalsaaidca parampreet singh...\n",
       "3    thank you very much to euecho for allocating  ...\n",
       "4    gempa \\n  \\n    km timurlaut deiyaipapua\\n  km...\n",
       "..                                                 ...\n",
       "495                      earthquake shakes tajikistan \n",
       "496  northport send emergency relief for turkiye sy...\n",
       "497  ‚ö°Ô∏èthe kazakh government and citizens have expr...\n",
       "498  üî¥ terremoto magnitudo  in tagikistan il moment...\n",
       "499  deprem sonrasƒ± evsiz kalan vatanda≈ülarƒ±mƒ±zƒ±n m...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.DataFrame(df.content.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning - second round\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‚Äò‚Äô‚Äú‚Äù‚Ä¶_-]', '', text)\n",
    "    text = re.sub('\\n\\t', '', text)\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>temblor   m    km wnw of murghob tajikistan  e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üåÑ sismo earthquake  m  utc on land r√≠o segundo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mandeeptoronto rt khalsaaidca parampreet singh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you very much to euecho for allocating  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gempa \\n  \\n    km timurlaut deiyaipapua\\n  km...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>earthquake shakes tajikistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>northport send emergency relief for turkiye sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>‚ö°Ô∏èthe kazakh government and citizens have expr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>üî¥ terremoto magnitudo  in tagikistan il moment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>deprem sonrasƒ± evsiz kalan vatanda≈ülarƒ±mƒ±zƒ±n m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content\n",
       "0    temblor   m    km wnw of murghob tajikistan  e...\n",
       "1    üåÑ sismo earthquake  m  utc on land r√≠o segundo...\n",
       "2    mandeeptoronto rt khalsaaidca parampreet singh...\n",
       "3    thank you very much to euecho for allocating  ...\n",
       "4    gempa \\n  \\n    km timurlaut deiyaipapua\\n  km...\n",
       "..                                                 ...\n",
       "495                      earthquake shakes tajikistan \n",
       "496  northport send emergency relief for turkiye sy...\n",
       "497  ‚ö°Ô∏èthe kazakh government and citizens have expr...\n",
       "498  üî¥ terremoto magnitudo  in tagikistan il moment...\n",
       "499  deprem sonrasƒ± evsiz kalan vatanda≈ülarƒ±mƒ±zƒ±n m...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.DataFrame(data_clean.content.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/eesoonhang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/eesoonhang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# dataframe text preprocessing\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.corpus.words.words() intends to remove non-english words\n",
    "mystopwords = stopwords.words(\"english\") + ['https']\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [WNlemma.lemmatize(t.lower()) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in mystopwords]\n",
    "    tokens = [t for t in tokens if len(t) >= 3]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the content column\n",
    "df['tokenize_content'] = df['content'].apply(preprocess)\n",
    "\n",
    "# convert list of tokens to string of tokens\n",
    "df['tokenize_content'] = df['tokenize_content'].apply(lambda x: \",\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>like</th>\n",
       "      <th>retweet</th>\n",
       "      <th>content</th>\n",
       "      <th>date_formatted</th>\n",
       "      <th>tokenize_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-23 20:42:34+08:00</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Temblor -  M 4.6 - 89 km WNW of Murghob, Tajik...</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>temblor,4.6,wnw,murghob,tajikistan,http,//t.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-23 20:42:01+08:00</td>\n",
       "      <td>&lt;a href=\"http://reddit.com/r/EEW\" rel=\"nofollo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>üåÑ #Sismo! #Earthquake! 4.2 M, 12:20 UTC on lan...</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>sismo,earthquake,4.2,12:20,utc,land,r√≠o,segund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-23 20:41:48+08:00</td>\n",
       "      <td>&lt;a href=\"https://ifttt.com\" rel=\"nofollow\"&gt;IFT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mandeeptoronto: RT @khalsaaidca: Parampreet Si...</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>mandeeptoronto,khalsaaidca,parampreet,singh,ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-23 20:41:42+08:00</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you very much to @eu_echo for allocating...</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>thank,much,eu_echo,allocating,mill,eur,alert,f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-23 20:41:41+08:00</td>\n",
       "      <td>&lt;a href=\"https://biston.web.id\" rel=\"nofollow\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#Gempa Mag:3.7\\n 23-Feb-2023 19:36:42WIB\\n Lok...</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>gempa,mag:3.7,23-feb-2023,19:36:42wib,lok:4.04...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  \\\n",
       "0 2023-02-23 20:42:34+08:00   \n",
       "1 2023-02-23 20:42:01+08:00   \n",
       "2 2023-02-23 20:41:48+08:00   \n",
       "3 2023-02-23 20:41:42+08:00   \n",
       "4 2023-02-23 20:41:41+08:00   \n",
       "\n",
       "                                              source  like  retweet  \\\n",
       "0  <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...     0        0   \n",
       "1  <a href=\"http://reddit.com/r/EEW\" rel=\"nofollo...     0        0   \n",
       "2  <a href=\"https://ifttt.com\" rel=\"nofollow\">IFT...     0        0   \n",
       "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...     1        0   \n",
       "4  <a href=\"https://biston.web.id\" rel=\"nofollow\"...     0        0   \n",
       "\n",
       "                                             content date_formatted  \\\n",
       "0  Temblor -  M 4.6 - 89 km WNW of Murghob, Tajik...     2023-02-23   \n",
       "1  üåÑ #Sismo! #Earthquake! 4.2 M, 12:20 UTC on lan...     2023-02-23   \n",
       "2  mandeeptoronto: RT @khalsaaidca: Parampreet Si...     2023-02-23   \n",
       "3  Thank you very much to @eu_echo for allocating...     2023-02-23   \n",
       "4  #Gempa Mag:3.7\\n 23-Feb-2023 19:36:42WIB\\n Lok...     2023-02-23   \n",
       "\n",
       "                                    tokenize_content  \n",
       "0  temblor,4.6,wnw,murghob,tajikistan,http,//t.co...  \n",
       "1  sismo,earthquake,4.2,12:20,utc,land,r√≠o,segund...  \n",
       "2  mandeeptoronto,khalsaaidca,parampreet,singh,ta...  \n",
       "3  thank,much,eu_echo,allocating,mill,eur,alert,f...  \n",
       "4  gempa,mag:3.7,23-feb-2023,19:36:42wib,lok:4.04...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('earthquake', 1204), ('tajikistan', 293), ('turkey', 184), ('magnitude', 159), ('utc', 149), ('sismo', 128), ('deprem', 119), ('syria', 111), ('ago', 107), ('china', 103), ('chile', 84), ('temblor', 83), ('info', 79), ('min', 74), ('strikes', 72), ('usgs', 72), ('terremoto', 65), ('6.8', 63), ('7.2', 60), ('reports', 54), ('near', 52), ('depth', 51), ('region', 49), ('2023', 47), ('app', 42), ('pakistan', 42), ('tajikistanearthquake', 42), ('border', 40), ('people', 40), ('2/23/23', 39), ('2023/02/23', 38), ('2023-02-23', 37), ('‡§Æ‡•á‡§Ç', 37), ('february', 37), ('//t.co/rblvnxzyn8', 36), ('murghob', 35), ('feb', 35), ('hit', 35), ('news', 34), ('gempa', 32), ('new', 31), ('‡§≠‡•Ç‡§ï‡§Ç‡§™', 31), ('felt', 30), ('islamabad', 29), ('turkeyearthquake', 28), ('time', 27), ('4.5', 27), ('affected', 26), ('t√ºrkiye', 26), ('aid', 25)]\n"
     ]
    }
   ],
   "source": [
    "# visualize the tokens frequency distribution\n",
    "toks =[t for t in [term.lower() for row in (nltk.word_tokenize(text) for text in df['content']) for term in row] if (t not in mystopwords and len(t)>=3)]\n",
    "fd = nltk.FreqDist(toks)\n",
    "print(fd.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out non-english text\n",
    "'''\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "corpus = []\n",
    "for text in df['content']:\n",
    "    doc = nlp(text)\n",
    "    detect_language = doc._.language\n",
    "    if (detect_language['language'] == 'en') and (detect_language['score'] > 0.5):\n",
    "        corpus.append(text)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
